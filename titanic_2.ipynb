{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.example import get_titanic_data_and_model\n",
    "train_df, test_df, model = get_titanic_data_and_model()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.context import DataContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Dr</td>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "58   female        S    Miss       2   5.0      1      2  27.75         1\n",
       "339    male        S      Mr       1  45.0      0      0  35.50         0\n",
       "138    male        S      Mr       3  16.0      0      0   9.22         0\n",
       "317    male        S      Dr       2  54.0      0      0  14.00         0\n",
       "827    male        C  Master       2   1.0      0      2  37.00         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Miss</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>male</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context = DataContext(\n",
    "    name=\"my_first_dataset\",\n",
    "    version=\"0.0.1\",\n",
    "    target=\"Survived\",\n",
    "    testing_data=test_df,\n",
    "    training_data=train_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import ModelValidator\n",
    "from examples.classification_titanic.custom_scorer import custom_scorers  # see ./custom_scorer.py example\n",
    "from examples.classification_titanic.slicing_functions import slicing_functions  # see ./slicing_functions.py for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator = ModelValidator(\n",
    "    data=data_context, model=model, custom_scorers=custom_scorers, slicing_functions=slicing_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_performance_against_threshold'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7184466019417476</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">295</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance calculated.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_performance_against_threshold'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'metric'\u001b[0m: \u001b[32m'recall'\u001b[0m, \u001b[32m'threshold'\u001b[0m: \u001b[1;36m0.7\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'performance'\u001b[0m: \u001b[1;36m0.7184466019417476\u001b[0m, \u001b[32m'sample_size'\u001b[0m: \u001b[1;36m295\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on \u001b[0m\n",
       "\u001b[32many of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom \u001b[0m\n",
       "\u001b[32mtrubrics.validations import ModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_performance_against_threshold\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"recall\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnthreshold\u001b[0m\u001b[32m=\u001b[0m\u001b[32m0\u001b[0m\u001b[32m.8\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nA\u001b[0m\n",
       "\u001b[32mrgs:\\nmetric: performance metric name defined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer fed in \u001b[0m\n",
       "\u001b[32mwhen initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must \u001b[0m\n",
       "\u001b[32mattain.\\ndataset: the name of a dataset from the DataContext \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'testing_data\\', \\'training_data\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\\ndata_slice: \u001b[0m\n",
       "\u001b[32mthe name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of \u001b[0m\n",
       "\u001b[32mthe validation. Can be either \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'error\\', \\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. If None, defaults to \u001b[0m\n",
       "\u001b[32m\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model \u001b[0m\n",
       "\u001b[32mperformance calculated.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_performance_against_threshold'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'data_slice'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'children'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance calculated.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_performance_against_threshold'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'metric'\u001b[0m: \u001b[32m'recall'\u001b[0m, \u001b[32m'threshold'\u001b[0m: \u001b[1;36m0.8\u001b[0m, \u001b[32m'data_slice'\u001b[0m: \u001b[32m'children'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'performance'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'sample_size'\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on \u001b[0m\n",
       "\u001b[32many of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom \u001b[0m\n",
       "\u001b[32mtrubrics.validations import ModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_performance_against_threshold\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"recall\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnthreshold\u001b[0m\u001b[32m=\u001b[0m\u001b[32m0\u001b[0m\u001b[32m.8\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nA\u001b[0m\n",
       "\u001b[32mrgs:\\nmetric: performance metric name defined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer fed in \u001b[0m\n",
       "\u001b[32mwhen initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must \u001b[0m\n",
       "\u001b[32mattain.\\ndataset: the name of a dataset from the DataContext \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'testing_data\\', \\'training_data\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\\ndata_slice: \u001b[0m\n",
       "\u001b[32mthe name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of \u001b[0m\n",
       "\u001b[32mthe validation. Can be either \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'error\\', \\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. If None, defaults to \u001b[0m\n",
       "\u001b[32m\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model \u001b[0m\n",
       "\u001b[32mperformance calculated.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_performance_against_threshold'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'my_custom_loss'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.203</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">295</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nA</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance calculated.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_performance_against_threshold'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'metric'\u001b[0m: \u001b[32m'my_custom_loss'\u001b[0m, \u001b[32m'threshold'\u001b[0m: \u001b[1;36m-0.7\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'performance'\u001b[0m: \u001b[1;36m-0.203\u001b[0m, \u001b[32m'sample_size'\u001b[0m: \u001b[1;36m295\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on \u001b[0m\n",
       "\u001b[32many of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom \u001b[0m\n",
       "\u001b[32mtrubrics.validations import ModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_performance_against_threshold\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"recall\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnthreshold\u001b[0m\u001b[32m=\u001b[0m\u001b[32m0\u001b[0m\u001b[32m.8\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nA\u001b[0m\n",
       "\u001b[32mrgs:\\nmetric: performance metric name defined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer fed in \u001b[0m\n",
       "\u001b[32mwhen initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must \u001b[0m\n",
       "\u001b[32mattain.\\ndataset: the name of a dataset from the DataContext \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'testing_data\\', \\'training_data\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\\ndata_slice: \u001b[0m\n",
       "\u001b[32mthe name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of \u001b[0m\n",
       "\u001b[32mthe validation. Can be either \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'error\\', \\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. If None, defaults to \u001b[0m\n",
       "\u001b[32m\\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model \u001b[0m\n",
       "\u001b[32mperformance calculated.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_performance_between_train_and_test'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'train_performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9832635983263598</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'test_performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7184466019417476</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'train_sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">596</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'test_sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">295</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation comparing training and test data scores.**\\n\\nScores the test set and the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">train set in the DataContext, and validates whether the test score is inferior to but also within a certain range </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the train score. Can be used to validate for overfitting\\non the training set.\\n\\nExample:\\n```py\\nfrom </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_performance_between_train_and_test(\\nmetric=\"recall\",\\nthreshold=0.3\\n)\\n```</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in when initialising the ModelValidator object.\\nthreshold: a positive value representing the maximum allowable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">difference between the train and test score.\\ndata_slice: the name of the data slice, specified in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either [\\'error\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'warning\\', \\'experiment\\']. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a results dictionary giving the model\\'s performance on test and train sets.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_performance_between_train_and_test'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'metric'\u001b[0m: \u001b[32m'recall'\u001b[0m, \u001b[32m'threshold'\u001b[0m: \u001b[1;36m0.3\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'train_performance'\u001b[0m: \u001b[1;36m0.9832635983263598\u001b[0m,\n",
       "        \u001b[32m'test_performance'\u001b[0m: \u001b[1;36m0.7184466019417476\u001b[0m,\n",
       "        \u001b[32m'train_sample_size'\u001b[0m: \u001b[1;36m596\u001b[0m,\n",
       "        \u001b[32m'test_sample_size'\u001b[0m: \u001b[1;36m295\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation comparing training and test data scores.**\\n\\nScores the test set and the\u001b[0m\n",
       "\u001b[32mtrain set in the DataContext, and validates whether the test score is inferior to but also within a certain range \u001b[0m\n",
       "\u001b[32mof the train score. Can be used to validate for overfitting\\non the training set.\\n\\nExample:\\n```py\\nfrom \u001b[0m\n",
       "\u001b[32mtrubrics.validations import ModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_performance_between_train_and_test\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"recall\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnthreshold\u001b[0m\u001b[32m=\u001b[0m\u001b[32m0\u001b[0m\u001b[32m.3\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\u001b[0m\n",
       "\u001b[32m\\n\\nArgs:\\nmetric: performance metric name defined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer fed \u001b[0m\n",
       "\u001b[32min when initialising the ModelValidator object.\\nthreshold: a positive value representing the maximum allowable \u001b[0m\n",
       "\u001b[32mdifference between the train and test score.\\ndata_slice: the name of the data slice, specified in the \u001b[0m\n",
       "\u001b[32mslicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'error\\', \u001b[0m\n",
       "\u001b[32m\\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With \u001b[0m\n",
       "\u001b[32ma results dictionary giving the model\\'s performance on test and train sets.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_test_performance_against_dummy'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'dummy_performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6508474576271186</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test_performance'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7966101694915254</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sample_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">295</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation of testing data versus a dummy baseline model.**\\n\\nTrains a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DummyClassifier / DummyRegressor from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[sklearn](https://scikit-learn.org/stable/modules/classes.html?highlight=dummy#module-sklearn.dummy) and compares </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance against the model on the test set.\\n\\nExample:\\n```py\\nfrom trubrics.validations import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_test_performance_against_dummy(\\nmetric=\"accuracy\",\\nstrategy=\"stratified\"\\n</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fed in when initialising the ModelValidator object.\\nstrategy: strategy of scikit-learns dummy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model.\\ndummy_kwargs: kwargs to be passed to dummy model.\\ndata_slice: the name of the data slice, specified in the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either [\\'error\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'warning\\', \\'experiment\\']. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a results dictionary giving the model\\'s actual performance on the test set and the dummy model\\'s performance.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_test_performance_against_dummy'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'metric'\u001b[0m: \u001b[32m'accuracy'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'dummy_performance'\u001b[0m: \u001b[1;36m0.6508474576271186\u001b[0m, \u001b[32m'test_performance'\u001b[0m: \u001b[1;36m0.7966101694915254\u001b[0m, \u001b[32m'sample_size'\u001b[0m: \u001b[1;36m295\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation of testing data versus a dummy baseline model.**\\n\\nTrains a \u001b[0m\n",
       "\u001b[32mDummyClassifier / DummyRegressor from \u001b[0m\n",
       "\u001b[32m[\u001b[0m\u001b[32msklearn\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mhttps://scikit-learn.org/stable/modules/classes.html?\u001b[0m\u001b[32mhighlight\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdummy\u001b[0m\u001b[32m#module-sklearn.dummy\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and compares \u001b[0m\n",
       "\u001b[32mperformance against the model on the test set.\\n\\nExample:\\n```py\\nfrom trubrics.validations import \u001b[0m\n",
       "\u001b[32mModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_test_performance_against_dummy\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"accuracy\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnstrategy\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"stratified\"\u001b[0m\u001b[32m\\n\u001b[0m\n",
       "\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer\u001b[0m\n",
       "\u001b[32mfed in when initialising the ModelValidator object.\\nstrategy: strategy of scikit-learns dummy \u001b[0m\n",
       "\u001b[32mmodel.\\ndummy_kwargs: kwargs to be passed to dummy model.\\ndata_slice: the name of the data slice, specified in the\u001b[0m\n",
       "\u001b[32mslicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\'error\\', \u001b[0m\n",
       "\u001b[32m\\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With \u001b[0m\n",
       "\u001b[32ma results dictionary giving the model\\'s actual performance on the test set and the dummy model\\'s performance.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_performance_std_across_slices'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'metric'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'std_threshold'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.07</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'testing_data'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'data_slices'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'male'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'children'</span><span style=\"font-weight: bold\">]</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'performances'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'testing_data_male'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8078817733990148</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'testing_data_children'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9090909090909091</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'sample_sizes'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'testing_data_male'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">203</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'testing_data_children'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Performance validation comparing data slices.**\\n\\nValidates that a list of model performances </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on different data slices from a given dataset has a lower\\nstandard deviation than a given threshold </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nslicing_functions = {\"female\": lambda </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">x: x[x[\"Sex\"]==\"female\"], \"male\": lambda x: x[x[\"Sex\"]==\"male\"]}\\nmodel_validator = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ModelValidator(data=data_context, model=model, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">slicing_functions=slicing_functions)\\nmodel_validator.validate_performance_std_across_slices(\\nmetric=\"recall\",\\nda</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">taset=\"training_data\",\\ndata_slices=[\"male\", </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"female\"],\\nstd_threshold=0.05,\\ninclude_global_performance=True\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">object.\\ndata_slices: a list of of data slices, specified in the slicing_functions parameter of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ModelValidator.\\nstd_threshold: the standard deviation threshold that must be superior to the standard deviation of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">all data slice performances.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\'training_data\\'}.\\ninclude_global_performance: whether or not to include the dataset global performance in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">list.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">defaults to \\'error\\'.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_performance_std_across_slices'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'metric'\u001b[0m: \u001b[32m'accuracy'\u001b[0m,\n",
       "            \u001b[32m'std_threshold'\u001b[0m: \u001b[1;36m0.07\u001b[0m,\n",
       "            \u001b[32m'dataset'\u001b[0m: \u001b[32m'testing_data'\u001b[0m,\n",
       "            \u001b[32m'data_slices'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'male'\u001b[0m, \u001b[32m'children'\u001b[0m\u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'performances'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'testing_data_male'\u001b[0m: \u001b[1;36m0.8078817733990148\u001b[0m, \u001b[32m'testing_data_children'\u001b[0m: \u001b[1;36m0.9090909090909091\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'sample_sizes'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'testing_data_male'\u001b[0m: \u001b[1;36m203\u001b[0m, \u001b[32m'testing_data_children'\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Performance validation comparing data slices.**\\n\\nValidates that a list of model performances \u001b[0m\n",
       "\u001b[32mon different data slices from a given dataset has a lower\\nstandard deviation than a given threshold \u001b[0m\n",
       "\u001b[32mvalue.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nslicing_functions = \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"female\": lambda \u001b[0m\n",
       "\u001b[32mx: x\u001b[0m\u001b[32m[\u001b[0m\u001b[32mx\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Sex\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m==\"female\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, \"male\": lambda x: x\u001b[0m\u001b[32m[\u001b[0m\u001b[32mx\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Sex\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m==\"male\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nmodel_validator = \u001b[0m\n",
       "\u001b[32mModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mslicing_functions\u001b[0m\u001b[32m=\u001b[0m\u001b[32mslicing_functions\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_performance_std_across_slices\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mnmetric\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"recall\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnda\u001b[0m\n",
       "\u001b[32mtaset\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"training_data\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mndata_slices\u001b[0m\u001b[32m=\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"male\", \u001b[0m\n",
       "\u001b[32m\"female\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnstd_threshold\u001b[0m\u001b[32m=\u001b[0m\u001b[32m0\u001b[0m\u001b[32m.05,\\\u001b[0m\u001b[32mninclude_global_performance\u001b[0m\u001b[32m=\u001b[0m\u001b[32mTrue\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nArgs:\\nmetric: performance metric name \u001b[0m\n",
       "\u001b[32mdefined in sklearn \u001b[0m\u001b[32m(\u001b[0m\u001b[32msklearn.metrics.SCORERS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or in a custom scorer fed in when initialising the ModelValidator \u001b[0m\n",
       "\u001b[32mobject.\\ndata_slices: a list of of data slices, specified in the slicing_functions parameter of \u001b[0m\n",
       "\u001b[32mModelValidator.\\nstd_threshold: the standard deviation threshold that must be superior to the standard deviation of\u001b[0m\n",
       "\u001b[32mall data slice performances.\\ndataset: the name of a dataset from the DataContext \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'testing_data\\', \u001b[0m\n",
       "\u001b[32m\\'training_data\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\\ninclude_global_performance: whether or not to include the dataset global performance in the \u001b[0m\n",
       "\u001b[32mlist.\\nseverity: severity of the validation. Can be either \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'error\\', \\'warning\\', \\'experiment\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m. If None, \u001b[0m\n",
       "\u001b[32mdefaults to \\'error\\'.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "performance = [\n",
    "    # validate overall model performance with a manual threshold\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.7),\n",
    "\n",
    "    # validate model performance on a specific data slice\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.8, data_slice=\"children\"),\n",
    "\n",
    "    # validate model performance with a custom metric \"my_custom_loss\"\n",
    "    model_validator.validate_performance_against_threshold(metric=\"my_custom_loss\", threshold=-0.7),\n",
    "\n",
    "    # validate model performance difference between train and test sets (overfit)\n",
    "    model_validator.validate_performance_between_train_and_test(metric=\"recall\", threshold=0.3),\n",
    "\n",
    "    # validate model performance against an sklearn dummy model\n",
    "    model_validator.validate_test_performance_against_dummy(metric=\"accuracy\"),\n",
    "\n",
    "    # validate model performance is regular between various data slices\n",
    "    model_validator.validate_performance_std_across_slices(metric=\"accuracy\", std_threshold=0.07, dataset=\"testing_data\", data_slices=[\"male\", \"children\"]),\n",
    "]\n",
    "\n",
    "for performance_val in performance:\n",
    "    rich.print(performance_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:26:36.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n",
      "\u001b[32m2023-05-24 18:26:38.717\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for training_data with 10 permutations.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_feature_in_top_n_important_features'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'testing_data'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'feature'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Sex'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'top_n_features'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'feature_importance_ranking'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'feature_importance'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Sex'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08237288135593218</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Embarked'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0037288135593219972</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039322033898305075</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Pclass'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019999999999999973</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Age'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019999999999999973</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'SibSp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014237288135593208</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Parch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.000677966101694949</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Fare'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012542372881355911</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">top n most important features. For calculation of feature importance we are using sklearn\\'s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_feature_in_top_n_important_features(\\ndataset=\"testing_data\",\\nfeature=\"feat</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ure_a\",\\ntop_n_features=2,\\n)\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important features that the named feature must be ranked in. E.g. if\\ntop_n_features=2, the feature must be within </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance on {\\'testing_data\\', \\'training_data\\'}.\\npermutation_kwargs: kwargs to pass into the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dictionary giving the actual feature importance ranking.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_feature_in_top_n_important_features'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'dataset'\u001b[0m: \u001b[32m'testing_data'\u001b[0m, \u001b[32m'feature'\u001b[0m: \u001b[32m'Sex'\u001b[0m, \u001b[32m'top_n_features'\u001b[0m: \u001b[1;36m3\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'feature_importance_ranking'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "        \u001b[32m'feature_importance'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Sex'\u001b[0m: \u001b[1;36m0.08237288135593218\u001b[0m,\n",
       "            \u001b[32m'Embarked'\u001b[0m: \u001b[1;36m0.0037288135593219972\u001b[0m,\n",
       "            \u001b[32m'Title'\u001b[0m: \u001b[1;36m0.039322033898305075\u001b[0m,\n",
       "            \u001b[32m'Pclass'\u001b[0m: \u001b[1;36m0.019999999999999973\u001b[0m,\n",
       "            \u001b[32m'Age'\u001b[0m: \u001b[1;36m0.019999999999999973\u001b[0m,\n",
       "            \u001b[32m'SibSp'\u001b[0m: \u001b[1;36m0.014237288135593208\u001b[0m,\n",
       "            \u001b[32m'Parch'\u001b[0m: \u001b[1;36m-0.000677966101694949\u001b[0m,\n",
       "            \u001b[32m'Fare'\u001b[0m: \u001b[1;36m0.012542372881355911\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the \u001b[0m\n",
       "\u001b[32mtop n most important features. For calculation of feature importance we are using sklearn\\'s \u001b[0m\n",
       "\u001b[32mpermutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = \u001b[0m\n",
       "\u001b[32mModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_feature_in_top_n_important_features\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mndataset\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"testing_data\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnfeature\u001b[0m\u001b[32m=\u001b[0m\u001b[32m\"feat\u001b[0m\n",
       "\u001b[32mure_a\"\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mntop_n_features\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2\u001b[0m\u001b[32m,\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most \u001b[0m\n",
       "\u001b[32mimportant features that the named feature must be ranked in. E.g. if\\\u001b[0m\u001b[32mntop_n_features\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2\u001b[0m\u001b[32m, the feature must be within \u001b[0m\n",
       "\u001b[32mthe top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature \u001b[0m\n",
       "\u001b[32mimportance on \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'testing_data\\', \\'training_data\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m.\\npermutation_kwargs: kwargs to pass into the \u001b[0m\n",
       "\u001b[32msklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results \u001b[0m\n",
       "\u001b[32mdictionary giving the actual feature importance ranking.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_feature_importance_between_train_and_test'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'top_n_features'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'training_feature_importance'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Parch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.000677966101694949</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Embarked'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0037288135593219972</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Fare'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.012542372881355911</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'SibSp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.014237288135593208</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Age'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019999999999999973</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Pclass'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.019999999999999973</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.039322033898305075</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Sex'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.08237288135593218</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'testing_feature_importance'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Parch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0241610738255033</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'SibSp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.061241610738254994</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Embarked'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06493288590604022</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09161073825503355</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Pclass'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.10687919463087243</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Age'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11644295302013422</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Fare'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14043624161073823</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Sex'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1583892617449664</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"**Permutation feature importance validation between train and test sets.**\\n\\nValidates that the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ranking of top n features is the same for both test and train sets. For calculation of feature importance we are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using sklearn's permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ModelValidator\\nmodel_validator = ModelValidator(data=data_context, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model=model)\\nmodel_validator.validate_feature_importance_between_train_and_test(\\ntop_n_features=1\\n)\\n```\\n\\nArgs</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">:\\ntop_n_features: the number of most important features to consider for comparison between train and test sets. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">E.g. if top_n_features=2, the train and test sets must have the same 2 most important features, in the same </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">order.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">importance ranking.\\n\"</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_feature_importance_between_train_and_test'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'top_n_features'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'error'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'training_feature_importance'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Parch'\u001b[0m: \u001b[1;36m-0.000677966101694949\u001b[0m,\n",
       "            \u001b[32m'Embarked'\u001b[0m: \u001b[1;36m0.0037288135593219972\u001b[0m,\n",
       "            \u001b[32m'Fare'\u001b[0m: \u001b[1;36m0.012542372881355911\u001b[0m,\n",
       "            \u001b[32m'SibSp'\u001b[0m: \u001b[1;36m0.014237288135593208\u001b[0m,\n",
       "            \u001b[32m'Age'\u001b[0m: \u001b[1;36m0.019999999999999973\u001b[0m,\n",
       "            \u001b[32m'Pclass'\u001b[0m: \u001b[1;36m0.019999999999999973\u001b[0m,\n",
       "            \u001b[32m'Title'\u001b[0m: \u001b[1;36m0.039322033898305075\u001b[0m,\n",
       "            \u001b[32m'Sex'\u001b[0m: \u001b[1;36m0.08237288135593218\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'testing_feature_importance'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Parch'\u001b[0m: \u001b[1;36m0.0241610738255033\u001b[0m,\n",
       "            \u001b[32m'SibSp'\u001b[0m: \u001b[1;36m0.061241610738254994\u001b[0m,\n",
       "            \u001b[32m'Embarked'\u001b[0m: \u001b[1;36m0.06493288590604022\u001b[0m,\n",
       "            \u001b[32m'Title'\u001b[0m: \u001b[1;36m0.09161073825503355\u001b[0m,\n",
       "            \u001b[32m'Pclass'\u001b[0m: \u001b[1;36m0.10687919463087243\u001b[0m,\n",
       "            \u001b[32m'Age'\u001b[0m: \u001b[1;36m0.11644295302013422\u001b[0m,\n",
       "            \u001b[32m'Fare'\u001b[0m: \u001b[1;36m0.14043624161073823\u001b[0m,\n",
       "            \u001b[32m'Sex'\u001b[0m: \u001b[1;36m0.1583892617449664\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m\"**Permutation feature importance validation between train and test sets.**\\n\\nValidates that the \u001b[0m\n",
       "\u001b[32mranking of top n features is the same for both test and train sets. For calculation of feature importance we are \u001b[0m\n",
       "\u001b[32musing sklearn's permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import \u001b[0m\n",
       "\u001b[32mModelValidator\\nmodel_validator = ModelValidator\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m=\u001b[0m\u001b[32mdata_context\u001b[0m\u001b[32m, \u001b[0m\n",
       "\u001b[32mmodel\u001b[0m\u001b[32m=\u001b[0m\u001b[32mmodel\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nmodel_validator.validate_feature_importance_between_train_and_test\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\u001b[0m\u001b[32mntop_n_features\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n```\\n\\nArgs\u001b[0m\n",
       "\u001b[32m:\\ntop_n_features: the number of most important features to consider for comparison between train and test sets. \u001b[0m\n",
       "\u001b[32mE.g. if \u001b[0m\u001b[32mtop_n_features\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2\u001b[0m\u001b[32m, the train and test sets must have the same 2 most important features, in the same \u001b[0m\n",
       "\u001b[32morder.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance \u001b[0m\n",
       "\u001b[32mfunction.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature \u001b[0m\n",
       "\u001b[32mimportance ranking.\\n\"\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainability = [\n",
    "    # validate that a feature is in the top N important features of permutation feature importances\n",
    "    model_validator.validate_feature_in_top_n_important_features(dataset=\"testing_data\", feature=\"Sex\", top_n_features=3),\n",
    "\n",
    "    # validate the top N features are the same in train and test sets (overfit)\n",
    "    model_validator.validate_feature_importance_between_train_and_test(top_n_features=2),\n",
    "]\n",
    "\n",
    "for explainability_val in explainability:\n",
    "    rich.print(explainability_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.classification_titanic.custom_validator import CustomValidator  # see custom_validator.py for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom_validator = CustomValidator(data=data_context, model=model, custom_scorers=custom_scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Validation</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'validate_master_age'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">validation_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'args'</span>: <span style=\"font-weight: bold\">()</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'age_limit_master'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">passed</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">severity</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'warning'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'errors_df'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Sex'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Embarked'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Pclass'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Age'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'SibSp'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Parch'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Fare'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Survived'</span>: <span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">explanation</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Validate that passengers with the title \"master\" are younger than a certain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">age\\n\\nArgs:\\nage_limit_master: cut off value for master\\n\\nReturns:\\nTrue for success, false otherwise. With a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results dictionary giving dict of errors.\\n'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mValidation\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mvalidation_type\u001b[0m=\u001b[32m'validate_master_age'\u001b[0m,\n",
       "    \u001b[33mvalidation_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'args'\u001b[0m: \u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'age_limit_master'\u001b[0m: \u001b[1;36m13\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpassed\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mseverity\u001b[0m=\u001b[32m'warning'\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'errors_df'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Sex'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Embarked'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Title'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Pclass'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Age'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'SibSp'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Parch'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Fare'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Survived'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexplanation\u001b[0m=\u001b[32m'Validate that passengers with the title \"master\" are younger than a certain \u001b[0m\n",
       "\u001b[32mage\\n\\nArgs:\\nage_limit_master: cut off value for master\\n\\nReturns:\\nTrue for success, false otherwise. With a \u001b[0m\n",
       "\u001b[32mresults dictionary giving dict of errors.\\n'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom = [\n",
    "    model_custom_validator.validate_master_age(age_limit_master=13, severity=\"warning\")\n",
    "]\n",
    "\n",
    "rich.print(custom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import Trubric\n",
    "\n",
    "validations =  performance + explainability + custom\n",
    "\n",
    "trubric = Trubric(\n",
    "    name=\"classification_trubric\",\n",
    "    model_name=\"my_model\",\n",
    "    model_version=\"0.0.1\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    metadata={\"some parameter\": \"xyz\"},\n",
    "    tags=[\"nb-demo\"],\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:27:52.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrubrics.validations.dataclass\u001b[0m:\u001b[36msave_local\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mTrubric saved to ./my_first_trubric.json.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save trubric to a local .json\n",
    "trubric.save_local(path=\"./my_first_trubric.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations.run import TrubricRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trubric_from_file = Trubric.parse_file(\"./my_first_trubric.json\")\n",
    "\n",
    "trubric_run_context = TrubricRun(\n",
    "    data_context=data_context,\n",
    "    model=model,\n",
    "    trubric=trubric_from_file,\n",
    "    tags=[\"nb-demo-new\"],\n",
    "    custom_validator=CustomValidator,\n",
    "    custom_scorers=custom_scorers,\n",
    "    slicing_functions=slicing_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_performance_against_threshold <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">......................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_performance_against_threshold \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_performance_against_threshold <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">......................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_performance_against_threshold \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_performance_against_threshold <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">......................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_performance_against_threshold \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_performance_between_train_and_test <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">.................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_performance_between_train_and_test \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m.\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_test_performance_against_dummy <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">.....................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_test_performance_against_dummy \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m..\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_performance_std_across_slices <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">......................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_performance_std_across_slices \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:28:19.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_feature_in_top_n_important_features <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_feature_in_top_n_important_features \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:28:20.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for training_data with 10 permutations.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_feature_importance_between_train_and_test <span style=\"font-weight: bold\">[</span>ERROR<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">..........................................</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">FAIL</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_feature_importance_between_train_and_test \u001b[1m[\u001b[0mERROR\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[1;31mFAIL\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">validate_master_age <span style=\"font-weight: bold\">[</span>WARNING<span style=\"font-weight: bold\">]</span><span style=\"color: #d0d0d0; text-decoration-color: #d0d0d0\">.......................................................................</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PASS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "validate_master_age \u001b[1m[\u001b[0mWARNING\u001b[1m]\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m...\u001b[0m\u001b[38;5;252m..\u001b[0m\u001b[1;32mPASS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_trubric = trubric_run_context.set_new_trubric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:29:16.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrubrics.validations.dataclass\u001b[0m:\u001b[36msave_local\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mTrubric saved to ./classification_trubric.json.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_trubric.save_local()  # save the new trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2578223027.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    trubrics run no-save-ui run-context-path titanic-example-trubric trubric-output-file-path \"cli_demo_trubric.json\"\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "trubrics run no-save-ui run-context-path titanic-example-trubric trubric-output-file-path \"cli_demo_trubric.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"TRUBRICS_CONFIG_EMAIL\"] = input(\"Enter your Trubrics account email:\")\n",
    "os.environ[\"TRUBRICS_CONFIG_PASSWORD\"] = getpass.getpass(\"Enter your password:\")\n",
    "os.environ[\"TRUBRICS_PROJECT_NAME\"] = input(\"Enter your Trubrics project name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataContext(name='my_dataset', version='v0.0.1', testing_data=        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1\n",
       "..      ...      ...     ...     ...   ...    ...    ...    ...       ...\n",
       "879  female        C     Mrs       1  56.0      0      1  83.16         1\n",
       "682    male        S      Mr       3  20.0      0      0   9.22         0\n",
       "509    male        S      Mr       3  26.0      0      0  56.50         1\n",
       "224    male        S      Mr       1  38.0      1      0  90.00         1\n",
       "386    male        S  Master       3   1.0      5      2  46.90         0\n",
       "\n",
       "[295 rows x 9 columns], target='Survived', training_data=None, minimum_functionality_data=None, features=['Sex', 'Embarked', 'Title', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], categorical_columns=None, business_columns=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator = ModelValidator(\n",
    "    data=data_context, model=model, custom_scorers=custom_scorers, slicing_functions=slicing_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data not specified in DataContext.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m performance \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m# validate overall model performance with a manual threshold\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m),\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[39m# validate model performance on a specific data slice\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, data_slice\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchildren\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[39m# validate model performance with a custom metric \"my_custom_loss\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy_custom_loss\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.7\u001b[39m),\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[39m# validate model performance difference between train and test sets (overfit)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     model_validator\u001b[39m.\u001b[39;49mvalidate_performance_between_train_and_test(metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m\"\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m),\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m     \u001b[39m# validate model performance against an sklearn dummy model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_test_performance_against_dummy(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m     \u001b[39m# validate model performance is regular between various data slices\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_std_across_slices(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, std_threshold\u001b[39m=\u001b[39m\u001b[39m0.07\u001b[39m, dataset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slices\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mchildren\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m performance_val \u001b[39min\u001b[39;00m performance:\n\u001b[0;32m     22\u001b[0m     rich\u001b[39m.\u001b[39mprint(performance_val)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\validation_output.py:20\u001b[0m, in \u001b[0;36mvalidation_output.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Validation:\n\u001b[1;32m---> 20\u001b[0m     output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     22\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         check_type(\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m, output, validation_output_type)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:375\u001b[0m, in \u001b[0;36mModelValidator.validate_performance_between_train_and_test\u001b[1;34m(self, metric, threshold, data_slice, severity)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39m@validation_output\u001b[39m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_performance_between_train_and_test\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     severity: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    345\u001b[0m ):\n\u001b[0;32m    346\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"**Performance validation comparing training and test data scores.**\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[39m    Scores the test set and the train set in the DataContext, and validates whether the test score is \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39m        performance on test and train sets.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_performance_between_train_and_test(metric, threshold, data_slice)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:385\u001b[0m, in \u001b[0;36mModelValidator._validate_performance_between_train_and_test\u001b[1;34m(self, metric, threshold, data_slice)\u001b[0m\n\u001b[0;32m    383\u001b[0m test_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_data_context(metric, dataset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice\u001b[39m=\u001b[39mdata_slice)\n\u001b[0;32m    384\u001b[0m test_sample_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_slice_sizes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_renamed_dataset(\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice)]\n\u001b[1;32m--> 385\u001b[0m train_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score_data_context(metric, dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining_data\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_slice\u001b[39m=\u001b[39;49mdata_slice)\n\u001b[0;32m    386\u001b[0m train_sample_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_slice_sizes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_renamed_dataset(\u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice)]\n\u001b[0;32m    388\u001b[0m passed \u001b[39m=\u001b[39m test_score \u001b[39m<\u001b[39m train_score \u001b[39mand\u001b[39;00m test_score \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m train_score \u001b[39m-\u001b[39m threshold\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:592\u001b[0m, in \u001b[0;36mModelValidator._score_data_context\u001b[1;34m(self, metric, dataset, data_slice)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39melif\u001b[39;00m data_slice \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mX_train \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39my_train \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 592\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTraining data not specified in DataContext.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mX_train, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39my_train\n",
      "\u001b[1;31mValueError\u001b[0m: Training data not specified in DataContext."
     ]
    }
   ],
   "source": [
    "performance = [\n",
    "    # validate overall model performance with a manual threshold\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.7),\n",
    "\n",
    "    # validate model performance on a specific data slice\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.8, data_slice=\"children\"),\n",
    "\n",
    "    # validate model performance with a custom metric \"my_custom_loss\"\n",
    "    model_validator.validate_performance_against_threshold(metric=\"my_custom_loss\", threshold=-0.7),\n",
    "\n",
    "    # validate model performance difference between train and test sets (overfit)\n",
    "    model_validator.validate_performance_between_train_and_test(metric=\"recall\", threshold=0.3),\n",
    "\n",
    "    # validate model performance against an sklearn dummy model\n",
    "    model_validator.validate_test_performance_against_dummy(metric=\"accuracy\"),\n",
    "\n",
    "    # validate model performance is regular between various data slices\n",
    "    model_validator.validate_performance_std_across_slices(metric=\"accuracy\", std_threshold=0.07, dataset=\"testing_data\", data_slices=[\"male\", \"children\"]),\n",
    "]\n",
    "\n",
    "for performance_val in performance:\n",
    "    rich.print(performance_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import ModelValidator\n",
    "from examples.classification_titanic.custom_scorer import custom_scorers  # see ./custom_scorer.py example\n",
    "from examples.classification_titanic.slicing_functions import slicing_functions  # see ./slicing_functions.py for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build validations with ModelValidator\n",
    "model_validator = ModelValidator(data=data_context, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:06:30.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validations = [\n",
    "    model_validator.validate_performance_against_threshold(metric=\"accuracy\", threshold=0.7),\n",
    "    model_validator.validate_feature_in_top_n_important_features(feature=\"Age\", top_n_features=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_type': 'validate_performance_against_threshold',\n",
       " 'validation_kwargs': {'args': (),\n",
       "  'kwargs': {'metric': 'accuracy', 'threshold': 0.7}},\n",
       " 'passed': True,\n",
       " 'severity': 'error',\n",
       " 'result': {'performance': 0.7966101694915254, 'sample_size': 295},\n",
       " 'explanation': '**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Group validations into a Trubric\n",
    "trubric = Trubric(\n",
    "    name=\"my_first_trubric\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trubric(name='my_first_trubric', passed=None, total_passed=None, total_failed=None, failing_severity='error', data_context_name='my_dataset', data_context_version='v0.0.1', model_name=None, model_version=None, tags=[], run_by=None, git_commit=None, timestamp=None, metadata=None, validations=[Validation(validation_type='validate_performance_against_threshold', validation_kwargs={'args': (), 'kwargs': {'metric': 'accuracy', 'threshold': 0.7}}, passed=True, severity='error', result={'performance': 0.7966101694915254, 'sample_size': 295}, explanation='**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'), Validation(validation_type='validate_feature_in_top_n_important_features', validation_kwargs={'args': (), 'kwargs': {'feature': 'Age', 'top_n_features': 3}}, passed=True, severity='error', result={'feature_importance_ranking': 2, 'feature_importance': {'Sex': 0.08237288135593218, 'Embarked': 0.0037288135593219972, 'Title': 0.039322033898305075, 'Pclass': 0.019999999999999973, 'Age': 0.019999999999999973, 'SibSp': 0.014237288135593208, 'Parch': -0.000677966101694949, 'Fare': 0.012542372881355911}}, explanation='**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn\\'s permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_feature_in_top_n_important_features(\\ndataset=\"testing_data\",\\nfeature=\"feature_a\",\\ntop_n_features=2,\\n)\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most important features that the named feature must be ranked in. E.g. if\\ntop_n_features=2, the feature must be within the top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature importance on {\\'testing_data\\', \\'training_data\\'}.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature importance ranking.\\n')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:08:33.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrubrics.validations.dataclass\u001b[0m:\u001b[36msave_local\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mTrubric saved to ./my_first_trubric.json.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trubric.save_local(path=\"./my_first_trubric.json\")  # save trubric as a local .json file\n",
    "# trubric.save_ui()  # or to the Trubrics platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trubrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
