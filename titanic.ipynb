{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.context import DataContext\n",
    "from trubrics.example import get_titanic_data_and_model\n",
    "from trubrics.validations import ModelValidator, Trubric\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.example import get_titanic_data_and_model\n",
    "train_df, test_df, model = get_titanic_data_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Miss</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>male</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Dr</td>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Master</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "58   female        S    Miss       2   5.0      1      2  27.75         1\n",
       "339    male        S      Mr       1  45.0      0      0  35.50         0\n",
       "138    male        S      Mr       3  16.0      0      0   9.22         0\n",
       "317    male        S      Dr       2  54.0      0      0  14.00         0\n",
       "827    male        C  Master       2   1.0      0      2  37.00         1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context = DataContext(\n",
    "    name=\"my_first_dataset\",\n",
    "    version=\"0.0.1\",\n",
    "    target=\"Survived\",\n",
    "    testing_data=test_df,\n",
    "    training_data=train_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init DataContext\n",
    "data_context = DataContext(\n",
    "    testing_data=test_df,  # pandas dataframe of data to validate model on\n",
    "    target=\"Survived\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataContext(name='my_dataset', version='v0.0.1', testing_data=        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1\n",
       "..      ...      ...     ...     ...   ...    ...    ...    ...       ...\n",
       "879  female        C     Mrs       1  56.0      0      1  83.16         1\n",
       "682    male        S      Mr       3  20.0      0      0   9.22         0\n",
       "509    male        S      Mr       3  26.0      0      0  56.50         1\n",
       "224    male        S      Mr       1  38.0      1      0  90.00         1\n",
       "386    male        S  Master       3   1.0      5      2  46.90         0\n",
       "\n",
       "[295 rows x 9 columns], target='Survived', training_data=None, minimum_functionality_data=None, features=['Sex', 'Embarked', 'Title', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], categorical_columns=None, business_columns=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_validator = ModelValidator(\n",
    "    data=data_context, model=model, custom_scorers=custom_scorers, slicing_functions=slicing_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data not specified in DataContext.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m performance \u001b[39m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[39m# validate overall model performance with a manual threshold\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m),\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[39m# validate model performance on a specific data slice\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, data_slice\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchildren\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[39m# validate model performance with a custom metric \"my_custom_loss\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_against_threshold(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy_custom_loss\u001b[39m\u001b[39m\"\u001b[39m, threshold\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.7\u001b[39m),\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[39m# validate model performance difference between train and test sets (overfit)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     model_validator\u001b[39m.\u001b[39;49mvalidate_performance_between_train_and_test(metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrecall\u001b[39;49m\u001b[39m\"\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m),\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m     \u001b[39m# validate model performance against an sklearn dummy model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_test_performance_against_dummy(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m     \u001b[39m# validate model performance is regular between various data slices\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     model_validator\u001b[39m.\u001b[39mvalidate_performance_std_across_slices(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, std_threshold\u001b[39m=\u001b[39m\u001b[39m0.07\u001b[39m, dataset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slices\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mchildren\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m performance_val \u001b[39min\u001b[39;00m performance:\n\u001b[0;32m     22\u001b[0m     rich\u001b[39m.\u001b[39mprint(performance_val)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\validation_output.py:20\u001b[0m, in \u001b[0;36mvalidation_output.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Validation:\n\u001b[1;32m---> 20\u001b[0m     output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     22\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m         check_type(\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m, output, validation_output_type)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:375\u001b[0m, in \u001b[0;36mModelValidator.validate_performance_between_train_and_test\u001b[1;34m(self, metric, threshold, data_slice, severity)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39m@validation_output\u001b[39m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_performance_between_train_and_test\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     severity: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    345\u001b[0m ):\n\u001b[0;32m    346\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"**Performance validation comparing training and test data scores.**\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[39m    Scores the test set and the train set in the DataContext, and validates whether the test score is \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39m        performance on test and train sets.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_performance_between_train_and_test(metric, threshold, data_slice)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:385\u001b[0m, in \u001b[0;36mModelValidator._validate_performance_between_train_and_test\u001b[1;34m(self, metric, threshold, data_slice)\u001b[0m\n\u001b[0;32m    383\u001b[0m test_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_data_context(metric, dataset\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice\u001b[39m=\u001b[39mdata_slice)\n\u001b[0;32m    384\u001b[0m test_sample_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_slice_sizes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_renamed_dataset(\u001b[39m\"\u001b[39m\u001b[39mtesting_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice)]\n\u001b[1;32m--> 385\u001b[0m train_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score_data_context(metric, dataset\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtraining_data\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_slice\u001b[39m=\u001b[39;49mdata_slice)\n\u001b[0;32m    386\u001b[0m train_sample_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_slice_sizes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_renamed_dataset(\u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m, data_slice)]\n\u001b[0;32m    388\u001b[0m passed \u001b[39m=\u001b[39m test_score \u001b[39m<\u001b[39m train_score \u001b[39mand\u001b[39;00m test_score \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m train_score \u001b[39m-\u001b[39m threshold\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\model\\base.py:592\u001b[0m, in \u001b[0;36mModelValidator._score_data_context\u001b[1;34m(self, metric, dataset, data_slice)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[39melif\u001b[39;00m data_slice \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mX_train \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39my_train \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 592\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTraining data not specified in DataContext.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mX_train, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39my_train\n",
      "\u001b[1;31mValueError\u001b[0m: Training data not specified in DataContext."
     ]
    }
   ],
   "source": [
    "performance = [\n",
    "    # validate overall model performance with a manual threshold\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.7),\n",
    "\n",
    "    # validate model performance on a specific data slice\n",
    "    model_validator.validate_performance_against_threshold(metric=\"recall\", threshold=0.8, data_slice=\"children\"),\n",
    "\n",
    "    # validate model performance with a custom metric \"my_custom_loss\"\n",
    "    model_validator.validate_performance_against_threshold(metric=\"my_custom_loss\", threshold=-0.7),\n",
    "\n",
    "    # validate model performance difference between train and test sets (overfit)\n",
    "    model_validator.validate_performance_between_train_and_test(metric=\"recall\", threshold=0.3),\n",
    "\n",
    "    # validate model performance against an sklearn dummy model\n",
    "    model_validator.validate_test_performance_against_dummy(metric=\"accuracy\"),\n",
    "\n",
    "    # validate model performance is regular between various data slices\n",
    "    model_validator.validate_performance_std_across_slices(metric=\"accuracy\", std_threshold=0.07, dataset=\"testing_data\", data_slices=[\"male\", \"children\"]),\n",
    "]\n",
    "\n",
    "for performance_val in performance:\n",
    "    rich.print(performance_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.validations import ModelValidator\n",
    "from examples.classification_titanic.custom_scorer import custom_scorers  # see ./custom_scorer.py example\n",
    "from examples.classification_titanic.slicing_functions import slicing_functions  # see ./slicing_functions.py for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build validations with ModelValidator\n",
    "model_validator = ModelValidator(data=data_context, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:06:30.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validations = [\n",
    "    model_validator.validate_performance_against_threshold(metric=\"accuracy\", threshold=0.7),\n",
    "    model_validator.validate_feature_in_top_n_important_features(feature=\"Age\", top_n_features=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_type': 'validate_performance_against_threshold',\n",
       " 'validation_kwargs': {'args': (),\n",
       "  'kwargs': {'metric': 'accuracy', 'threshold': 0.7}},\n",
       " 'passed': True,\n",
       " 'severity': 'error',\n",
       " 'result': {'performance': 0.7966101694915254, 'sample_size': 295},\n",
       " 'explanation': '**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Group validations into a Trubric\n",
    "trubric = Trubric(\n",
    "    name=\"my_first_trubric\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trubric(name='my_first_trubric', passed=None, total_passed=None, total_failed=None, failing_severity='error', data_context_name='my_dataset', data_context_version='v0.0.1', model_name=None, model_version=None, tags=[], run_by=None, git_commit=None, timestamp=None, metadata=None, validations=[Validation(validation_type='validate_performance_against_threshold', validation_kwargs={'args': (), 'kwargs': {'metric': 'accuracy', 'threshold': 0.7}}, passed=True, severity='error', result={'performance': 0.7966101694915254, 'sample_size': 295}, explanation='**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'), Validation(validation_type='validate_feature_in_top_n_important_features', validation_kwargs={'args': (), 'kwargs': {'feature': 'Age', 'top_n_features': 3}}, passed=True, severity='error', result={'feature_importance_ranking': 2, 'feature_importance': {'Sex': 0.08237288135593218, 'Embarked': 0.0037288135593219972, 'Title': 0.039322033898305075, 'Pclass': 0.019999999999999973, 'Age': 0.019999999999999973, 'SibSp': 0.014237288135593208, 'Parch': -0.000677966101694949, 'Fare': 0.012542372881355911}}, explanation='**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn\\'s permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_feature_in_top_n_important_features(\\ndataset=\"testing_data\",\\nfeature=\"feature_a\",\\ntop_n_features=2,\\n)\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most important features that the named feature must be ranked in. E.g. if\\ntop_n_features=2, the feature must be within the top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature importance on {\\'testing_data\\', \\'training_data\\'}.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature importance ranking.\\n')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:08:33.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtrubrics.validations.dataclass\u001b[0m:\u001b[36msave_local\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mTrubric saved to ./my_first_trubric.json.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trubric.save_local(path=\"./my_first_trubric.json\")  # save trubric as a local .json file\n",
    "# trubric.save_ui()  # or to the Trubrics platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trubrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
