{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.context import DataContext\n",
    "from trubrics.example import get_titanic_data_and_model\n",
    "from trubrics.validations import ModelValidator, Trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_df, model = get_titanic_data_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Miss</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>male</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init DataContext\n",
    "data_context = DataContext(\n",
    "    testing_data=test_df,  # pandas dataframe of data to validate model on\n",
    "    target=\"Survived\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataContext(name='my_dataset', version='v0.0.1', testing_data=        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1\n",
       "..      ...      ...     ...     ...   ...    ...    ...    ...       ...\n",
       "879  female        C     Mrs       1  56.0      0      1  83.16         1\n",
       "682    male        S      Mr       3  20.0      0      0   9.22         0\n",
       "509    male        S      Mr       3  26.0      0      0  56.50         1\n",
       "224    male        S      Mr       1  38.0      1      0  90.00         1\n",
       "386    male        S  Master       3   1.0      5      2  46.90         0\n",
       "\n",
       "[295 rows x 9 columns], target='Survived', training_data=None, minimum_functionality_data=None, features=['Sex', 'Embarked', 'Title', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], categorical_columns=None, business_columns=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build validations with ModelValidator\n",
    "model_validator = ModelValidator(data=data_context, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 17:49:33.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validations = [\n",
    "    model_validator.validate_performance_against_threshold(metric=\"accuracy\", threshold=0.7),\n",
    "    model_validator.validate_feature_in_top_n_important_features(feature=\"Age\", top_n_features=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_type': 'validate_performance_against_threshold',\n",
       " 'validation_kwargs': {'args': (),\n",
       "  'kwargs': {'metric': 'accuracy', 'threshold': 0.7}},\n",
       " 'passed': True,\n",
       " 'severity': 'error',\n",
       " 'result': {'performance': 0.7966101694915254, 'sample_size': 295},\n",
       " 'explanation': '**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Group validations into a Trubric\n",
    "trubric = Trubric(\n",
    "    name=\"my_first_trubric\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trubric(name='my_first_trubric', passed=None, total_passed=None, total_failed=None, failing_severity='error', data_context_name='my_dataset', data_context_version='v0.0.1', model_name=None, model_version=None, tags=[], run_by=None, git_commit=None, timestamp=None, metadata=None, validations=[Validation(validation_type='validate_performance_against_threshold', validation_kwargs={'args': (), 'kwargs': {'metric': 'accuracy', 'threshold': 0.7}}, passed=True, severity='error', result={'performance': 0.7966101694915254, 'sample_size': 295}, explanation='**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'), Validation(validation_type='validate_feature_in_top_n_important_features', validation_kwargs={'args': (), 'kwargs': {'feature': 'Age', 'top_n_features': 3}}, passed=True, severity='error', result={'feature_importance_ranking': 2, 'feature_importance': {'Sex': 0.08237288135593218, 'Embarked': 0.0037288135593219972, 'Title': 0.039322033898305075, 'Pclass': 0.019999999999999973, 'Age': 0.019999999999999973, 'SibSp': 0.014237288135593208, 'Parch': -0.000677966101694949, 'Fare': 0.012542372881355911}}, explanation='**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn\\'s permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_feature_in_top_n_important_features(\\ndataset=\"testing_data\",\\nfeature=\"feature_a\",\\ntop_n_features=2,\\n)\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most important features that the named feature must be ranked in. E.g. if\\ntop_n_features=2, the feature must be within the top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature importance on {\\'testing_data\\', \\'training_data\\'}.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature importance ranking.\\n')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reference at 'refs/heads/main' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trubric\u001b[39m.\u001b[39;49msave_local(path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# save trubric as a local .json file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# trubric.save_ui()  # or to the Trubrics platform\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\dataclass.py:115\u001b[0m, in \u001b[0;36mTrubric.save_local\u001b[1;34m(self, path, raise_on_failure)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_local\u001b[39m(\u001b[39mself\u001b[39m, path: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, raise_on_failure: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_dynamic_fields()\n\u001b[0;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\dataclass.py:168\u001b[0m, in \u001b[0;36mTrubric.set_dynamic_fields\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestamp \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mtimestamp())\n\u001b[0;32m    167\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_commit \u001b[39m=\u001b[39m Repo(search_parent_directories\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mhead\u001b[39m.\u001b[39;49mobject\u001b[39m.\u001b[39mhexsha\n\u001b[0;32m    169\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidGitRepositoryError:\n\u001b[0;32m    170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit_commit \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\git\\refs\\symbolic.py:219\u001b[0m, in \u001b[0;36mSymbolicReference._get_object\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[39m    The object our ref currently refers to. Refs can be cached, they will\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39m    always point to the actual object as it gets re-created on each query\"\"\"\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m# have to be dynamic here as we may be a tag which can point to anything\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m# Our path will be resolved to the hexsha which will be used accordingly\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m \u001b[39mreturn\u001b[39;00m Object\u001b[39m.\u001b[39mnew_from_sha(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo, hex_to_bin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdereference_recursive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepo, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath)))\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\git\\refs\\symbolic.py:159\u001b[0m, in \u001b[0;36mSymbolicReference.dereference_recursive\u001b[1;34m(cls, repo, ref_path)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m:return: hexsha stored in the reference at the given ref_path, recursively dereferencing all\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    intermediate references as required\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m:param repo: the repository containing the reference at ref_path\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     hexsha, ref_path \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ref_info(repo, ref_path)\n\u001b[0;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m hexsha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m         \u001b[39mreturn\u001b[39;00m hexsha\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\git\\refs\\symbolic.py:210\u001b[0m, in \u001b[0;36mSymbolicReference._get_ref_info\u001b[1;34m(cls, repo, ref_path)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_ref_info\u001b[39m(\u001b[39mcls\u001b[39m, repo: \u001b[39m\"\u001b[39m\u001b[39mRepo\u001b[39m\u001b[39m\"\u001b[39m, ref_path: Union[PathLike, \u001b[39mNone\u001b[39;00m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mNone\u001b[39;00m], Tuple[\u001b[39mNone\u001b[39;00m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    rela_path points to, or None. target_ref_path is the reference we\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    point to, or None\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ref_info_helper(repo, ref_path)\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\git\\refs\\symbolic.py:193\u001b[0m, in \u001b[0;36mSymbolicReference._get_ref_info_helper\u001b[1;34m(cls, repo, ref_path)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[39m# END for each packed ref\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m# END handle packed refs\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mReference at \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ref_path)\n\u001b[0;32m    195\u001b[0m \u001b[39m# is it a reference ?\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m tokens[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mref:\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Reference at 'refs/heads/main' does not exist"
     ]
    }
   ],
   "source": [
    "trubric.save_local(path='.')  # save trubric as a local .json file\n",
    "# trubric.save_ui()  # or to the Trubrics platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trubrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
