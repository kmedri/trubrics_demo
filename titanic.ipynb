{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trubrics.context import DataContext\n",
    "from trubrics.example import get_titanic_data_and_model\n",
    "from trubrics.validations import ModelValidator, Trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_df, model = get_titanic_data_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>Miss</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>male</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>male</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init DataContext\n",
    "data_context = DataContext(\n",
    "    testing_data=test_df,  # pandas dataframe of data to validate model on\n",
    "    target=\"Survived\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataContext(name='my_dataset', version='v0.0.1', testing_data=        Sex Embarked   Title  Pclass   Age  SibSp  Parch   Fare  Survived\n",
       "751    male        S  Master       3   6.0      0      1  12.48         1\n",
       "90     male        S      Mr       3  29.0      0      0   8.05         0\n",
       "852  female        C    Miss       3   9.0      1      1  15.25         0\n",
       "718    male        Q      Mr       3   NaN      0      0  15.50         0\n",
       "36     male        C      Mr       3   NaN      0      0   7.23         1\n",
       "..      ...      ...     ...     ...   ...    ...    ...    ...       ...\n",
       "879  female        C     Mrs       1  56.0      0      1  83.16         1\n",
       "682    male        S      Mr       3  20.0      0      0   9.22         0\n",
       "509    male        S      Mr       3  26.0      0      0  56.50         1\n",
       "224    male        S      Mr       1  38.0      1      0  90.00         1\n",
       "386    male        S  Master       3   1.0      5      2  46.90         0\n",
       "\n",
       "[295 rows x 9 columns], target='Survived', training_data=None, minimum_functionality_data=None, features=['Sex', 'Embarked', 'Title', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], categorical_columns=None, business_columns=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build validations with ModelValidator\n",
    "model_validator = ModelValidator(data=data_context, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-05-24 18:00:38.235\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtrubrics.validations.model.base\u001b[0m:\u001b[36m_compute_permutation_feature_importance\u001b[0m:\u001b[36m629\u001b[0m - \u001b[34m\u001b[1mComputing permutation feature importance for testing_data with 10 permutations.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validations = [\n",
    "    model_validator.validate_performance_against_threshold(metric=\"accuracy\", threshold=0.7),\n",
    "    model_validator.validate_feature_in_top_n_important_features(feature=\"Age\", top_n_features=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_type': 'validate_performance_against_threshold',\n",
       " 'validation_kwargs': {'args': (),\n",
       "  'kwargs': {'metric': 'accuracy', 'threshold': 0.7}},\n",
       " 'passed': True,\n",
       " 'severity': 'error',\n",
       " 'result': {'performance': 0.7966101694915254, 'sample_size': 295},\n",
       " 'explanation': '**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Group validations into a Trubric\n",
    "trubric = Trubric(\n",
    "    name=\"my_first_trubric\",\n",
    "    data_context_name=data_context.name,\n",
    "    data_context_version=data_context.version,\n",
    "    validations=validations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trubric(name='my_first_trubric', passed=None, total_passed=None, total_failed=None, failing_severity='error', data_context_name='my_dataset', data_context_version='v0.0.1', model_name=None, model_version=None, tags=[], run_by=None, git_commit=None, timestamp=None, metadata=None, validations=[Validation(validation_type='validate_performance_against_threshold', validation_kwargs={'args': (), 'kwargs': {'metric': 'accuracy', 'threshold': 0.7}}, passed=True, severity='error', result={'performance': 0.7966101694915254, 'sample_size': 295}, explanation='**Performance validation versus a fixed threshold value.**\\n\\nCompares performance of a model on any of the datasets in the DataContext to a hard coded threshold value.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_performance_against_threshold(\\nmetric=\"recall\",\\nthreshold=0.8\\n)\\n```\\n\\nArgs:\\nmetric: performance metric name defined in sklearn (sklearn.metrics.SCORERS) or in a custom scorer fed in when initialising the ModelValidator object.\\nthreshold: the performance threshold that the model must attain.\\ndataset: the name of a dataset from the DataContext {\\'testing_data\\', \\'training_data\\'}.\\ndata_slice: the name of the data slice, specified in the slicing_functions parameter of ModelValidator.\\nseverity: severity of the validation. Can be either {\\'error\\', \\'warning\\', \\'experiment\\'}. If None, defaults to \\'error\\'.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual model performance calculated.\\n'), Validation(validation_type='validate_feature_in_top_n_important_features', validation_kwargs={'args': (), 'kwargs': {'feature': 'Age', 'top_n_features': 3}}, passed=True, severity='error', result={'feature_importance_ranking': 2, 'feature_importance': {'Sex': 0.08237288135593218, 'Embarked': 0.0037288135593219972, 'Title': 0.039322033898305075, 'Pclass': 0.019999999999999973, 'Age': 0.019999999999999973, 'SibSp': 0.014237288135593208, 'Parch': -0.000677966101694949, 'Fare': 0.012542372881355911}}, explanation='**Feature importance validation for top n features.**\\n\\nValidates that a given feature is in the top n most important features. For calculation of feature importance we are using sklearn\\'s permutation_importance.\\n\\nExample:\\n```py\\nfrom trubrics.validations import ModelValidator\\nmodel_validator = ModelValidator(data=data_context, model=model)\\nmodel_validator.validate_feature_in_top_n_important_features(\\ndataset=\"testing_data\",\\nfeature=\"feature_a\",\\ntop_n_features=2,\\n)\\n```\\n\\nArgs:\\nfeature: feature to assess.\\ntop_n_features: the number of most important features that the named feature must be ranked in. E.g. if\\ntop_n_features=2, the feature must be within the top two most important features.\\ndataset: the name of a dataset from the DataContext to calculate feature importance on {\\'testing_data\\', \\'training_data\\'}.\\npermutation_kwargs: kwargs to pass into the sklearn.inspection.permutation_importance function.\\n\\nReturns:\\nTrue for success, false otherwise. With a results dictionary giving the actual feature importance ranking.\\n')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'c:\\\\Users\\\\kmedr\\\\My Drive\\\\Projects\\\\trubrics_demo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trubric\u001b[39m.\u001b[39;49msave_local(path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# save trubric as a local .json file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trubric\u001b[39m.\u001b[39msave_ui()  \u001b[39m# or to the Trubrics platform\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kmedr\\anaconda3\\envs\\trubrics\\Lib\\site-packages\\trubrics\\validations\\dataclass.py:118\u001b[0m, in \u001b[0;36mTrubric.save_local\u001b[1;34m(self, path, raise_on_failure)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 118\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(Path(path)\u001b[39m.\u001b[39;49mabsolute(), \u001b[39m\"\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    119\u001b[0m     file\u001b[39m.\u001b[39mwrite(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson(indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[0;32m    120\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrubric saved to \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'c:\\\\Users\\\\kmedr\\\\My Drive\\\\Projects\\\\trubrics_demo'"
     ]
    }
   ],
   "source": [
    "trubric.save_local(path='.')  # save trubric as a local .json file\n",
    "trubric.save_ui()  # or to the Trubrics platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trubrics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
